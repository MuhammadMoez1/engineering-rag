#!/usr/bin/env python3
"""Verify that all cost optimizations are active."""

import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

print("=" * 60)
print("üîç Cost Optimization Verification")
print("=" * 60)

# Check OpenAI settings
print("\n1Ô∏è‚É£  OpenAI Model Configuration:")
model = os.getenv("OPENAI_MODEL", "NOT SET")
embedding_model = os.getenv("OPENAI_EMBEDDING_MODEL", "NOT SET")
max_tokens = os.getenv("OPENAI_MAX_TOKENS", "NOT SET")

if model == "gpt-4o-mini":
    print(f"   ‚úÖ GPT Model: {model} (COST-OPTIMIZED)")
    print(f"      Cost: $0.15/1M input, $0.60/1M output")
elif model == "gpt-4-turbo-preview":
    print(f"   ‚ö†Ô∏è  GPT Model: {model} (EXPENSIVE)")
    print(f"      Cost: $10/1M input, $30/1M output")
    print(f"      üí° Consider switching to gpt-4o-mini for 97% savings")
else:
    print(f"   ‚ùì GPT Model: {model}")

if embedding_model == "text-embedding-3-small":
    print(f"   ‚úÖ Embedding Model: {embedding_model} (COST-OPTIMIZED)")
    print(f"      Cost: $0.02/1M tokens")
elif embedding_model == "text-embedding-3-large":
    print(f"   ‚ö†Ô∏è  Embedding Model: {embedding_model} (EXPENSIVE)")
    print(f"      Cost: $0.13/1M tokens")
    print(f"      üí° Consider switching to text-embedding-3-small for 85% savings")
else:
    print(f"   ‚ùì Embedding Model: {embedding_model}")

print(f"   ‚ÑπÔ∏è  Max Tokens: {max_tokens}")

# Check document processing settings
print("\n2Ô∏è‚É£  Document Processing Configuration:")
chunk_size = os.getenv("CHUNK_SIZE", "NOT SET")
chunk_overlap = os.getenv("CHUNK_OVERLAP", "NOT SET")

if chunk_size == "800":
    print(f"   ‚úÖ Chunk Size: {chunk_size} tokens (COST-OPTIMIZED)")
    print(f"      ~20% fewer chunks than 1000 tokens")
elif chunk_size == "1000":
    print(f"   ‚ö†Ô∏è  Chunk Size: {chunk_size} tokens")
    print(f"      üí° Consider reducing to 800 for 20% savings")
else:
    print(f"   ‚ùì Chunk Size: {chunk_size} tokens")

print(f"   ‚ÑπÔ∏è  Chunk Overlap: {chunk_overlap} tokens")

# Check RAG settings
print("\n3Ô∏è‚É£  RAG Configuration:")
top_k = os.getenv("RETRIEVAL_TOP_K", "NOT SET")
cache_enabled = os.getenv("ENABLE_QUERY_CACHE", "NOT SET")
cache_ttl = os.getenv("CACHE_TTL_SECONDS", "NOT SET")

if top_k == "3":
    print(f"   ‚úÖ Retrieval Top-K: {top_k} documents (COST-OPTIMIZED)")
    print(f"      ~40% less context sent to GPT vs 5 documents")
elif top_k == "5":
    print(f"   ‚ö†Ô∏è  Retrieval Top-K: {top_k} documents")
    print(f"      üí° Consider reducing to 3 for 40% savings")
else:
    print(f"   ‚ùì Retrieval Top-K: {top_k} documents")

if cache_enabled == "True":
    print(f"   ‚úÖ Query Cache: ENABLED (SAVES 30-50%)")
    print(f"      Cache TTL: {cache_ttl} seconds")
else:
    print(f"   ‚ö†Ô∏è  Query Cache: DISABLED")
    print(f"      üí° Enable caching for 30-50% savings on repeated queries")

# Calculate expected costs
print("\n4Ô∏è‚É£  Expected Monthly Costs (558MB docs, 2000 queries):")

if (model == "gpt-4o-mini" and 
    embedding_model == "text-embedding-3-small" and 
    chunk_size == "800" and 
    top_k == "3" and 
    cache_enabled == "True"):
    print("   ‚úÖ FULLY OPTIMIZED CONFIGURATION")
    print("   üìä Estimated Cost: ~$2.84/month")
    print("      ‚Ä¢ Document processing: $1.44 (one-time)")
    print("      ‚Ä¢ Queries: $1.40/month")
    print("   üí∞ Savings: ~$169/month (98% reduction)")
elif (model == "gpt-4-turbo-preview" and 
      embedding_model == "text-embedding-3-large"):
    print("   ‚ö†Ô∏è  STANDARD CONFIGURATION (EXPENSIVE)")
    print("   üìä Estimated Cost: ~$172/month")
    print("      ‚Ä¢ Document processing: $12 (one-time)")
    print("      ‚Ä¢ Queries: $160/month")
else:
    print("   ‚ÑπÔ∏è  MIXED CONFIGURATION")
    print("   üìä Estimated Cost: Varies based on settings")
    print("      Check COST_OPTIMIZATION.md for details")

# Overall status
print("\n" + "=" * 60)
optimizations = []
warnings = []

if model == "gpt-4o-mini":
    optimizations.append("GPT-4o-mini")
else:
    warnings.append("Switch to GPT-4o-mini")

if embedding_model == "text-embedding-3-small":
    optimizations.append("Small embeddings")
else:
    warnings.append("Switch to text-embedding-3-small")

if chunk_size == "800":
    optimizations.append("Optimized chunks")
else:
    warnings.append("Reduce chunk size to 800")

if top_k == "3":
    optimizations.append("Optimized retrieval")
else:
    warnings.append("Reduce top_k to 3")

if cache_enabled == "True":
    optimizations.append("Query caching")
else:
    warnings.append("Enable query caching")

if not warnings:
    print("üéâ ALL OPTIMIZATIONS ACTIVE!")
    print(f"   Active: {', '.join(optimizations)}")
    print("   Status: Ready for production")
    print("   Expected savings: 98% vs standard config")
else:
    print(f"‚úÖ Active optimizations: {', '.join(optimizations)}")
    print(f"‚ö†Ô∏è  Recommended: {', '.join(warnings)}")

print("=" * 60)

# Instructions
print("\nüìö Resources:")
print("   ‚Ä¢ Full details: COST_OPTIMIZATION.md")
print("   ‚Ä¢ Quick summary: OPTIMIZATION_SUMMARY.md")
print("   ‚Ä¢ Monitor usage: https://platform.openai.com/usage")
print("\nüîß To modify settings:")
print("   1. Edit: nano .env")
print("   2. Restart: ./stop.sh && ./start.sh")
print("\n‚úÖ System ready! Access UI at: http://localhost:8501")
print("=" * 60)

